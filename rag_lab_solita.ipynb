{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb8fd03",
   "metadata": {},
   "source": [
    "## Packages & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be11104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "\n",
    "from typing import List, Tuple  \n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader            # loads PDFs page-by-page and stores page metadata\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter     # Text splitter for documents\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings               # LLM for answering & Embedding tool to turn text into vectors\n",
    "from langchain_chroma import Chroma "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3ef16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd()\n",
    "PDF_DIR = BASE_DIR / \"data\"             # Store data PDFs \n",
    "CHROMA_DIR = BASE_DIR / \"chroma_db\"     # Store Chroma DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b3144",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df6dbd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/andrealunghini/Desktop/data_science/rag_lab_solita/data/foundations_of_data_science.pdf\n"
     ]
    }
   ],
   "source": [
    "pdf_paths = sorted(PDF_DIR.glob(\"*.pdf\"))\n",
    "\n",
    "for path in pdf_paths:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49954570",
   "metadata": {},
   "source": [
    "## Step 1: Ingest & Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac191163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 479 page-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_pdf_text(text:str) -> str: \n",
    "    \"We want pure text in chunks.\" \n",
    "    \"Cleaning blanks.\"\n",
    "    text = re.sub(r'\\n \\n', ' ', text) \n",
    "    text = re.sub(r'\\n\\n+', '\\n\\n', text) \n",
    "    text = re.sub(r' +', ' ', text) \n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def load_pdfs(paths: List[Path]) -> List[Document]:\n",
    "    all_docs: List[Document] = [] \n",
    "    for path in paths:\n",
    "        loader = PyPDFLoader(str(path))\n",
    "        docs = loader.load() \n",
    "        for d in docs:\n",
    "            d.metadata[\"source\"] = Path(d.metadata.get(\"source\",path)).name \n",
    "            d.page_content = clean_pdf_text(d.page_content)\n",
    "        all_docs.extend(docs)\n",
    "    return all_docs\n",
    "\n",
    "docs = load_pdfs(pdf_paths)\n",
    "print(f\"Loaded {len(docs)} page-documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9c43ff",
   "metadata": {},
   "source": [
    "## Step 2: Chunking & Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca9ce4c",
   "metadata": {},
   "source": [
    "### Embedding strategies\n",
    "- Fixed-size chunking with overlap\n",
    "- Semantic chunking: avoiding splitting a sentence in a semantically important part of the text\n",
    "- Recursive chunking: recurservly splitting into chunk\n",
    "- Document structured-based chunk: inherent structure of the text\n",
    "- LLM-based chunking: input to LLM and LLM generated chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67159288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY found.\n",
      "Using chat model: gpt-4o-mini\n",
      "Using embed model: text-embedding-3-small\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise RuntimeError(\n",
    "        \"OPENAI_API_KEY not found. Create a .env file (copy from .env.example) and set OPENAI_API_KEY.\"\n",
    "    )\n",
    "\n",
    "CHAT_MODEL = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4o-mini\")\n",
    "EMBED_MODEL = os.getenv(\"OPENAI_EMBED_MODEL\", \"text-embedding-3-small\")\n",
    "\n",
    "print(\"OPENAI_API_KEY found.\")              # never print your API keys :)\n",
    "print(f\"Using chat model: {CHAT_MODEL}\")\n",
    "print(f\"Using embed model: {EMBED_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38461723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "# Simplest and dumbest approach\n",
    "\n",
    "# Common sense: \"gambling\" that overlap with important sentence is \n",
    "# not longer than 150 characters. \n",
    "splitter = CharacterTextSplitter(chunk_size=1500, \n",
    "                                 chunk_overlap=150, \n",
    "                                 separator=\" \")\n",
    "\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Foundation of DS: this is a good candidate for Document-structured based chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ea03c8",
   "metadata": {},
   "source": [
    "# Step 3: Build Vector DB & Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b9db66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
